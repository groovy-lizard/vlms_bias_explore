{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import open_clip\n",
    "from bias_explorer.operations.report import get_empty_report_dict, gen_dict_report\n",
    "from bias_explorer.utils import dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ViT-B-16\"\n",
    "data_source = \"openai\"\n",
    "preds_path = f\"clip_aggregation_preds_{model_name}_{data_source}.csv\"\n",
    "report_path = f\"clip_aggregation_report_{model_name}_{data_source}.csv\"\n",
    "img_path = f\"./data/fairface/embeddings/openCLIP/{model_name}/{data_source}/generated_img_embs.pkl\"\n",
    "val_path = \"./data/fairface/fface_val.csv\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embs = pd.read_pickle(img_path)\n",
    "fface_df = pd.read_csv(val_path)\n",
    "model, _, preprocessing = open_clip.create_model_and_transforms(\n",
    "    model_name, pretrained=data_source, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_templates = [\n",
    "    'a photo of a {}',\n",
    "    'a photo of a white {}',\n",
    "    'a photo of a young white {}',\n",
    "    'a photo of an old white {}',\n",
    "    'a photo of a black {}',\n",
    "    'a photo of a young black {}',\n",
    "    'a photo of an old black {}',\n",
    "    'a photo of a latino {}',\n",
    "    'a photo of a young latino {}',\n",
    "    'a photo of an old latino {}',\n",
    "    'a photo of a east asian {}',\n",
    "    'a photo of a young east asian {}',\n",
    "    'a photo of an old east asian {}',\n",
    "    'a photo of a southeast asian {}',\n",
    "    'a photo of a young southeast asian {}',\n",
    "    'a photo of an old southeast asian {}',\n",
    "    'a photo of an indian {}',\n",
    "    'a photo of a young indian {}',\n",
    "    'a photo of an old indian {}',\n",
    "    'a photo of a middle eastern {}',\n",
    "    'a photo of a young middle eastern {}',\n",
    "    'a photo of an old middle eastern {}'\n",
    "]\n",
    "\n",
    "gender_classes = ['man', 'woman']\n",
    "\n",
    "preds_classes = ['Male', 'Female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 32.78it/s]\n"
     ]
    }
   ],
   "source": [
    "def zeroshot_classifier(classnames, templates):\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        prompts = []\n",
    "        for classname in tqdm(classnames):\n",
    "            texts = [template.format(classname)\n",
    "                     for template in templates]  # format with class\n",
    "            prompts.append(texts)\n",
    "            texts = open_clip.tokenize(texts).cuda()  # tokenize\n",
    "            class_embeddings = model.encode_text(texts)  # embed with text encoder\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding)\n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
    "    return zeroshot_weights, prompts\n",
    "\n",
    "\n",
    "zeroshot_weights, prompts = zeroshot_classifier(gender_classes, clip_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = []\n",
    "for prompt in prompts:\n",
    "    for p in prompt:\n",
    "        plist.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a photo of a man',\n",
       " 'a photo of a white man',\n",
       " 'a photo of a young white man',\n",
       " 'a photo of an old white man',\n",
       " 'a photo of a black man',\n",
       " 'a photo of a young black man',\n",
       " 'a photo of an old black man',\n",
       " 'a photo of a latino man',\n",
       " 'a photo of a young latino man',\n",
       " 'a photo of an old latino man',\n",
       " 'a photo of a east asian man',\n",
       " 'a photo of a young east asian man',\n",
       " 'a photo of an old east asian man',\n",
       " 'a photo of a southeast asian man',\n",
       " 'a photo of a young southeast asian man',\n",
       " 'a photo of an old southeast asian man',\n",
       " 'a photo of an indian man',\n",
       " 'a photo of a young indian man',\n",
       " 'a photo of an old indian man',\n",
       " 'a photo of a middle eastern man',\n",
       " 'a photo of a young middle eastern man',\n",
       " 'a photo of an old middle eastern man',\n",
       " 'a photo of a woman',\n",
       " 'a photo of a white woman',\n",
       " 'a photo of a young white woman',\n",
       " 'a photo of an old white woman',\n",
       " 'a photo of a black woman',\n",
       " 'a photo of a young black woman',\n",
       " 'a photo of an old black woman',\n",
       " 'a photo of a latino woman',\n",
       " 'a photo of a young latino woman',\n",
       " 'a photo of an old latino woman',\n",
       " 'a photo of a east asian woman',\n",
       " 'a photo of a young east asian woman',\n",
       " 'a photo of an old east asian woman',\n",
       " 'a photo of a southeast asian woman',\n",
       " 'a photo of a young southeast asian woman',\n",
       " 'a photo of an old southeast asian woman',\n",
       " 'a photo of an indian woman',\n",
       " 'a photo of a young indian woman',\n",
       " 'a photo of an old indian woman',\n",
       " 'a photo of a middle eastern woman',\n",
       " 'a photo of a young middle eastern woman',\n",
       " 'a photo of an old middle eastern woman']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds_dict = {}\n",
    "    fnames = []\n",
    "    preds = []\n",
    "    for _, emb in img_embs.iterrows():\n",
    "        name = emb['file']\n",
    "        img_features = emb['embeddings']\n",
    "        image_features = torch.from_numpy(img_features).to(device)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "        text_probs = logits.softmax(dim=-1)\n",
    "        top_probs, top_labels = text_probs.cpu().topk(1, dim=-1)\n",
    "        pindex = top_labels.cpu().numpy().item()\n",
    "        fnames.append(name)\n",
    "        preds.append(preds_classes[pindex])\n",
    "    preds_dict['file'] = fnames\n",
    "    preds_dict['gender_preds'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(data=preds_dict)\n",
    "new_df = fface_df.set_index('file').join(preds_df.set_index('file'))\n",
    "new_df.drop(columns=['service_test'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(preds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = dataloader.load_df(preds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_dict = get_empty_report_dict(new_df, \"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_dict = gen_dict_report(new_df, \"OpenAI\", \"accuracy\", rep_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep_df = pd.DataFrame(rep_dict)\n",
    "rep_df.to_csv(report_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
