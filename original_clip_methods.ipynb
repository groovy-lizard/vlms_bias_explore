{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import open_clip\n",
    "from bias_explorer.utils import dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"ViT-B-16\"\n",
    "data_source = \"openai\"\n",
    "preds_path = f\"clip_aggregation_preds_{model_name}_{data_source}.csv\"\n",
    "report_path = f\"clip_aggregation_report_{model_name}_{data_source}.csv\"\n",
    "img_path = f\"./data/fairface/embeddings/openCLIP/{model_name}/{data_source}/generated_img_embs.pkl\"\n",
    "val_path = \"./data/fairface/fface_val.csv\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_embs = pd.read_pickle(img_path)\n",
    "fface_df = pd.read_csv(val_path)\n",
    "model, _, preprocessing = open_clip.create_model_and_transforms(\n",
    "    model_name, pretrained=data_source, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_templates = [\n",
    "    'a photo of a {}',\n",
    "    'a photo of a white {}',\n",
    "    'a photo of a young white {}',\n",
    "    'a photo of an old white {}',\n",
    "    'a photo of a black {}',\n",
    "    'a photo of a young black {}',\n",
    "    'a photo of an old black {}',\n",
    "    'a photo of a latino {}',\n",
    "    'a photo of a young latino {}',\n",
    "    'a photo of an old latino {}',\n",
    "    'a photo of a east asian {}',\n",
    "    'a photo of a young east asian {}',\n",
    "    'a photo of an old east asian {}',\n",
    "    'a photo of a southeast asian {}',\n",
    "    'a photo of a young southeast asian {}',\n",
    "    'a photo of an old southeast asian {}',\n",
    "    'a photo of an indian {}',\n",
    "    'a photo of a young indian {}',\n",
    "    'a photo of an old indian {}',\n",
    "    'a photo of a middle eastern {}',\n",
    "    'a photo of a young middle eastern {}',\n",
    "    'a photo of an old middle eastern {}'\n",
    "]\n",
    "\n",
    "gender_classes = ['man', 'woman']\n",
    "\n",
    "preds_classes = ['Male', 'Female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  5.52it/s]\n"
     ]
    }
   ],
   "source": [
    "def zeroshot_classifier(classnames, templates):\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        prompts = []\n",
    "        for classname in tqdm(classnames):\n",
    "            texts = [template.format(classname)\n",
    "                     for template in templates]  # format with class\n",
    "            prompts.append(texts)\n",
    "            texts = open_clip.tokenize(texts).cuda()  # tokenize\n",
    "            class_embeddings = model.encode_text(texts)  # embed with text encoder\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding)\n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
    "    return zeroshot_weights, prompts\n",
    "\n",
    "\n",
    "zeroshot_weights, prompts = zeroshot_classifier(gender_classes, clip_templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plist = []\n",
    "for prompt in prompts:\n",
    "    for p in prompt:\n",
    "        plist.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    preds_dict = {}\n",
    "    fnames = []\n",
    "    preds = []\n",
    "    for _, emb in img_embs.iterrows():\n",
    "        name = emb['file']\n",
    "        img_features = emb['embeddings']\n",
    "        image_features = torch.from_numpy(img_features).to(device)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "        text_probs = logits.softmax(dim=-1)\n",
    "        top_probs, top_labels = text_probs.cpu().topk(1, dim=-1)\n",
    "        pindex = top_labels.cpu().numpy().item()\n",
    "        fnames.append(name)\n",
    "        preds.append(preds_classes[pindex])\n",
    "    preds_dict['file'] = fnames\n",
    "    preds_dict['gender_preds'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(data=preds_dict)\n",
    "new_df = fface_df.set_index('file').join(preds_df.set_index('file'))\n",
    "new_df.drop(columns=['service_test'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"a photo of a{age}{race}{gender}\"\n",
    "age_labels = [\" young\", \" middle-aged\", \" old\", \"\"]\n",
    "race_labels = [\" black\", \" indian\", \" latino hispanic\",\n",
    "               \" middle eastern\", \" southeast asian\", \" east asian\", \" white\", \"\"]\n",
    "gender_labels = [\" woman\", \" man\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_prompts = [template.format(age=label, race=\"{race}\", gender=\"{gender}\") for label in age_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_prompts = []\n",
    "for ap in age_prompts:\n",
    "    for rl in race_labels:\n",
    "        race_prompts.append(ap.format(race=rl, gender=\"{gender}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prompts = []\n",
    "for rp in race_prompts:\n",
    "    for gl in gender_labels:\n",
    "        final_prompts.append(rp.format(gender=gl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"testlabels.json\", \"w\", encoding=\"utf-8\") as final:\n",
    "    json.dump(final_prompts, final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clipenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
